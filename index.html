

<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Artificial intelligence and Morality
Winter semester 2022 – 2023
</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta name="author" content="Yehuda Baharav" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="" />
<meta property="og:description" content="" />
<!-- End Jekyll SEO tag -->

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <meta property='og:title' content=''/>
    <meta property='og:description' content=''/>
    
    <link rel="stylesheet" href="stylesheet.css">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->

<!-- end custom head snippets -->

  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">Breaking the Bias: How to Recognize and Avoid Gender Bias in Recruiting Ads</h1>
      <h2 class="curse-name">Artificial intelligence and Morality
      </h2>
      <h2 class="semester-name">Winter semester 2022 – 2023</h2>
      <h2 class="reichman-name">Reichman University</h2>
      <!-- <h2 class="project-tagline" color=white><a href="website_homepage.htm">Home</a> <a href="website_hills.htm">Hills Pupil Tailored Website</a></h2> -->
      <h2 class="project-tagline"><a href="https://www.linkedin.com/in/yehuda-baharav/"  style="color:#ffffff">Yehuda Baharav - Efi Arazi School of Computer Science</a>
      <h2 class="project-tagline"><a href="https://www.linkedin.com/in/tomer-bar-tal-516127241"  style="color:#ffffff">Tomer Bar-Tal - Radzyner Law School</a>
      <h2 class="project-tagline"><a style="color:#ffffff">Shahaf Sherman - Radzyner Law School</a>
      <h2 class="project-tagline"><a style="color:#ffffff">Nizar - Radzyner Law School</a>     
    </header>

    <main id="content" class="main-content" role="main">

This paper was written by 
      <a href="https://www.linkedin.com/in/yehuda-baharav/">Yehuda Baharav</a>, <a href="https://www.linkedin.com/in/tomer-bar-tal-516127241">Tomer Bar-Tal</a>, Shahaf Sherman and Nizar from Reichman University.

<h2 id="Introduction">Introduction</h2>

<p>
Gender bias in job ads is a prevalent issue that has been extensively studied, particularly with regard to its impact on women  . Research has shown that women are more likely to apply for job postings only when they match the requirements almost perfectly, while men are more willing to apply even if they only partially meet the qualifications  
</p>
<p>
Bias in recruitment ads can take different forms, including the use of gender-specific terms like Policeman or Fireman, or the use of language that is more appealing to one gender over the other . There is also a more subtle type of bias in the wording or phrasing of ads, which may not be intentionally offensive but can still be alienating to one gender, typically women. Examples of such wording include the use of phrases like "must have" or "commitment". 
</p>
<p>
To address this problem, we will build a website that will assist recruiters in writing gender-neutral job ads. The goal of the website is to help recruiters recognize and eliminate any gender bias in their job ads, thus increasing diversity and inclusivity in the workplace. 
</p>

<h2 id="methodology">Methodology</h2>
<p>
  In the initial phase of our research, we opted to construct an NLP (Natural Language Processing) model aimed at identifying instances of gender bias in recruitment advertisements.
 </p>
<strong>Data:</strong>      
<p>We obtained the data from <a href="https://pandologic.com/">Pandologic</a>, which consists of 2 tables:</p>
 <p>     
<li>
<strong>Jobs table</strong>, containing the job title, field, and description.
Its size is about <strong>50K samples</strong>.
</li>
<li>
<strong>Job seekers table</strong> that includes their first and last names, email addresses, physical addresses, and other personal information. This table also indicates the jobs for which each job seeker applied.
Its size is about <strong>15M samples</strong>.
</li>
 </p>
  
<ol>
   <center><p><img width="500" src="images/image001.png" /></p></center>
</ol>
<ol>
<center><p><img width="500" src="images/image002.png" /></p></center>
</ol>
<p>To ensure <strong>information security</strong> and protect <strong>user privacy</strong>, I have not included all columns in the presentation of data. Additionally, (just for the presentation here) the columns have been deliberately mixed up and a fictitious CompPositionID has been used.</p>

<strong>Date engineering:</strong>
      
<p>We used the <a href="https://pypi.org/project/gender-guesser/">gender_guesser</a> library to extract the gender of users from the list of unique first names in the data. Afterward, we joined this information back to the users table.
  For example:</p>
<center><p><img width="500" src="images/image003.png" /></p></center>
      
 <strong>Bias index:</strong>    
 <p>
   To assess gender bias in job applications, we calculated a gender ratio by comparing the number of women who applied for each job to the number of men who applied for that same job. We then computed this ratio for each job field, such as lawyer or doctor, in our dataset.
</p>
<p> 
  Next, we developed a "bias index" that measures the variance or distance between the job ratio for each specific job and the average ratio for that job field. This index provides information about whether there is gender bias in the applicant pool for a particular job, relative to the overall ratio for that job field. The index ranges from -1 to 1, with -1 indicating bias against women, 1 indicating bias against men, and 0 indicating no bias.
</p>
<p> 
 To increase confidence in our analysis, we excluded job fields with fewer than 10 unique ads and jobs with fewer than 5 applicants.  
</p>

 <strong>Example:</strong> 
<p>
  Upon examining the jobs with the highest gender bias according to the index, we discovered several positions that exhibited a significant disparity in applications between men and women. For example, the attached  job of a Medical Office Manager demonstrated a gender gap,  with 51 applicants, all of them are male, while in average in this job field, 87% of the applicants are females.
</p>
<hr>    
<pre>
<strong>
Full Time Dental Office Manager - $25 to $30/hr + 401k, PTO & BONUSES
Our dental practice located in the adorable South Park area of San Diego, CA is seeking an experienced Dental Office Manager to <span style="background-color: #FFFF00">fearlessly</span> lead our team!
Our practice is <span style="background-color: #FFFF00">extremely busy and we don't have time to train</span>, so DENTAL OFFICE MANAGEMENT EXPERIENCE IS REQUIRED to be considered for this role.
For your <span style="background-color: #FFFF00">hard work and dedication</span> we offer:Competitive and generous pay 401KPaid Time OffPaid Holidays and Paid Sick LeaveQuarterly BonusesOur hours are Monday to Thursday 9AM to 6PM.
To be considered for our Dental Office Manager role please:
<span style="background-color: #FFFF00">Have at least</span> 3 years of office management experienceHave experience with Treatment PlanningHave HMO and PPO dental insurance billing experienceBe reliable and friendly.
Bilingual English/Spanish is a huge plus!This position is open now!  Apply today!
Skills:General Practice
</strong>
</pre>     
<hr>    

<p>Although this <strong>result may be considered an outlier</strong>, it is worth noting that bias likely persists in similar jobs, even if not to this extreme degree. It's important to acknowledge that the observed gender imbalance may be influenced by chance, <strong>but the overall trend remains significant</strong>.</p>
 <p><u>An interesting fact to note is that upon visiting the company's website, it was observed that the word 'fearlessly' has been removed from the job description since its initial posting:</u></p>     

<p><strong>EDA (exploratory data analysis):</strong></p>
      
<center><p><img width="500" src="images/image004.png" /></p></center>


<p>The graph clearly shows that the bias against women in distinct areas is nearly twice as much as that against men in the corresponding areas. To provide a better explanation, we will mirror the right side of the graph to the left side:</p>
<center><p><img width="500" src="images/image005_v2.png" /></p></center>

<strong>Transfer learning - NLP model:</strong>
<p>
Initially, I experimented with traditional machine learning techniques such as boosting. However, I quickly realized that this approach was not suitable, and therefore, we will not delve into it further. Instead, I opted for using a pre-trained model
<a href="https://huggingface.co/distilbert-base-uncased">distilbert-base-uncased</a> by dropping the last layer, and training it on my data. If required, I would also train a few more layers backwards.
</p>
<p>
  The rationale behind using a pre-trained model is that in deep learning, the semantic meaning becomes more complex as we move deeper into the layers. Hence, the initial layers can be used for nearly any purpose in the field, such as text analysis in our case. The only aspect that needs to be learned about our target is limited to the last few layers.
</p>
<p>
I will explain this concept using image analysis as it is difficult to illustrate visually with text analysis. In the initial stage, the model identifies more general features, such as differences between pixels in the image. Similarly, in text analysis, this could include identifying the language of the text or analyzing linking words. As the analysis progresses, the model delves deeper into more meaningful features, such as identifying the presence of a person in a specific area of the picture. In text analysis, this could mean analyzing the use of high language in a particular sentence compared to the sentences before and after it.
</p>
<ol>
  <center><div class="img-Sync-SGD-diagram">
    <img width="500" img src="images/image006.png" alt="Source: Image Recognition with Deep Neural Networks and its Use Cases" />
    <p>Source: <a href="https://www.altexsoft.com/blog/image-recognition-neural-networks-use-cases/">Image Recognition with Deep Neural Networks and its Use Cases</a></p>
</div></center>
</ol>
 <strong>Tokenizer:</strong>
 <p>To incorporate the description into the model, we needed to tokenize the text to extract features. Thanks to Hugging Face's AutoTokenizer, this process was automated according to the selected model, saving us a considerable amount of time.
      </p>
  <p>Tokenization is a crucial step in NLP, as it involves breaking down raw text into individual tokens (such as words or subwords) that can be further processed. However, selecting the right tokenizer can be challenging since different models and tasks may require different approaches.
      </p>
<p>AutoTokenizer solves this problem by automatically selecting the appropriate tokenizer based on the model architecture and task. By using AutoTokenizer, we were able to streamline the selection and configuration of the tokenizer, ultimately saving us time and effort.
  </p>
      
  <strong>Target definition and metrics: </strong>
<p>We explored various approaches for training our data. These included:</p>
 <p>     
<li>
Conducting a <strong>regression analysis on all available data</strong> and testing for general error (MAE). We also examined error concentration in problem areas, such as the end of the left tail where there is a bias. The advantage of this method is that we have a large amount of data, and we don't need to establish a threshold for identifying bias. However, the disadvantage is that the data is unbalanced, which could negatively impact performance in areas with bias.
  <p>The objective loss of this method was RMSE.</p>
</li>
<li>
Limiting our <strong>regression analysis to the area where bias exists</strong>, and balancing it with data of the same size from areas without bias. This approach balances the data, but it comes at the cost of losing some information.
  <p>The objective loss of this method was RMSE.</p>
</li>
<li>
Using a <strong>classification</strong> approach to train the model <strong>on the area where there is bias</strong> and balancing it with data of the same size from areas without bias. This method balances the data and may result in easier model training, but it requires establishing a threshold for identifying bias, and we still lose some data.
<p>Since the data is balanced in this case, we used RMSE as objective loss but also examined the confusion matrix of this method.</p>  
</li>
 </p>
 <p>
In summary, we explored different ways to train our data, each with its own set of advantages and disadvantages.
      </p>  
<h2 id="results">Results</h2>
      
<p>
  The initial model showed promising results, achieving over 90% accuracy across all methods. However, during one of the last tests, the model was run with a random target under the same distribution parameters as the original target, and the accuracy remained unchanged. After verifying that there was no correlation between the random target and the original target, my confidence in the model was lost, and it will not be deployed in our website.
      </p>
<ol>
   <center><p><img width="500" src="images/image007.png" /></p></center>
</ol>  
      
<p>
One possible explanation for the model's high accuracy on a random target is that there may be many similar, albeit not identical, texts in the data. The model learns these texts during training, allowing it to predict answers accurately during validation.
</p>
<p>
 A naive solution to this problem is to set a difference threshold, only allowing texts that differ beyond this threshold to be entered. In other words, duplicates will be downloaded even if they are not 100% identical. However, this solution has a time complexity of O(n^2), making it impractical for large datasets such as the one being used (n is ~50K). 
</p>
Although there may be smarter solutions, we do not have enough time to implement them. Therefore, we will use results from previous studies in the field (not necessarily related to machine learning) and recommendations from international organizations to create a list of words that may contain gender bias. This list will then be embedded into our website.      
<p>
The websites and studies that we will use include:  
</p>
<p>Source: <a href="https://eige.europa.eu/publications/gender-sensitive-communication/practical-tools/examples-common-gendered-nouns-alternatives"></a>Gender- sensitive Communication - European Institute for Gender Equality</p>
<p>Source: <a href="https://www.researchgate.net/publication/50303045_Evidence_That_Gendered_Wording_in_Job_Advertisements_Exists_and_Sustains_Gender_Inequality"></a>Evidence That Gendered Wording in Job Advertisements Exists and Sustains Gender Inequality</p>
 
<h2 id="legal-opinion">Legal Opinion</h2>
      
      
     
      
      
      
      
      
      
<h2 id="stochastic-gradient-descent">Stochastic Gradient Descent</h2>

<p>Before we dive into the implementation details and challenges, let’s first understand what stochastic gradient
    descent (SGD) is. Given a dataset D and a model with θ parameters, we’d like to minimize the parameterized empirical
    loss function, L, for a given (x,y) pairs in D, where x denotes the input sample while y is the output.</p>
<ol>
   <center><p><img width="500" src="images/loss_function.png" /></p></center>
</ol>
      
<p>Where l is the loss of a data point (x,y) for model θ. </p>
<p>A first-order stochastic optimization algorithm optimizes the loss function by iteratively updating θ using a
    stochastic gradient. Usually, a learning rate will be applied to avoid over or underfitting, and therefore,
    the SGD will be calculated as follows:</p>
<ol>
   <center><center><p><img width="500" src="images/SGD.png" /></p></center>
</ol>     
<p>where ɣ is the learning rate or step size at iteration.</p>
<p>A mini-batch version of the stochastic optimization algorithm computes the gradient over a mini-batch of size B
    instead of a single data point:</p>
<ol>
   <center><p><img width="500" src="images/mini_batch_size_b.png" /></p></center>
</ol>      
<h2 id="synchronous-stochastic-gradient-descent">Synchronous stochastic gradient descent</h2>
<p>Using distributed Synchronous Stochastic Gradient Descent (Sync-SGD), a root aggregator node splits the data into
    batches and fans-out requests to leaf nodes (worker machines) to process each batch and compute its gradient
    independently. Once all machines return their result, the root aggregator node averages the gradient and sends
    it back to the workers to update the model’s parameters. The root aggregator iterates over this process for a given
    number of epochs or based on a conversion condition.</p>
<ol>
  <center><div class="img-Sync-SGD-diagram">
    <img width="500" img src="images/Sync-SGD-diagram.png" alt="FIGURE 1: DISTRIBUTED SYNCHRONOUS STOCHASTIC GRADIENT DESCENT FLOW" />
    <p>FIGURE 1: DISTRIBUTED SYNCHRONOUS STOCHASTIC GRADIENT DESCENT FLOW</p>
</div></center>
</ol> 
<h2 id="Issues">Issues</h2> 
<p>In theory, distributing the computation onto T worker machines should give a performance improvement of xT. Yet, in
    reality, the performance improvement is rarely xT. The decline in efficiency is caused due to many reasons, where
    recent researches segment Stragglers= as the main root cause.</p>
 
      
<p>
Stragglers are tasks that run much slower than other workers. Slow stragglers may result from failing hardware,
    contention on shared underlying hardware resources in data centers, or even preemption by other jobs.
  <a href="https://arxiv.org/abs/1604.00981">Rafal et al. (2017)</a>
  conducted an experiment that calculated the time it takes to run a Sync-SGD using 100 workers and 19 parameters on
    the Inception model. These times are presented in Figure 2.
</p>
<ol>
<center><div class="img-cdf-time">
    <img width="500" img src="images/cdf_time.png" alt="cdf_time" />
    <p>FIGURE 2: THE EFFECT OF NUMBER OF WORKERS ON THE SYNC-SGD TRAINING TIME, by
       <a href="https://arxiv.org/abs/1604.00981"> Rafal et al. (2017)</a>
    </p>
</div></center>
</ol>     
      
 

     
<h2 id="solutions">Solutions</h2>

<p>
  The “arming race” for state-of-the-art machine learning models has led the industry to develop complex systems that require heavy computing to train.
  When it comes to such a large scale, any delay in the training cycle sum to a significant delay in the entire training process.
  To meet those high requirements and reduce the iteration time, researchers from academia and industry invest considerable time and
  resources in refining and streamlining the training process. we’ve gathered SOTA solutions for stragglers that
  improves the training time when using the Sync-SGD approach.
</p>     

<h3 id="drop-stragglers">drop-stragglers</h3> 
<p>
  Our initial approach to solving this issue was to drop the results of the stragglers and calculate the gradient using
    the results from the other workers. I supported this approach using two main hypotheses:
</p>
      
<ul class="ul-2">
  <li>Low ratio of stragglers to completed tasks — As fanning out to more workers, the number of stragglers increases,
      however, they only accommodate the ~98th percentile, and therefore their impact will be relatively small on the gradient on average.</li>
  <li>Reducing a small amount of data from a large-scale dataset has little effect — The Sync-SGD method is used for a
      large amount of data. Thus, removing the result of the small batches from the training process will have a negligible effect on the gradient on average.</li>
</ul> 
<p>
  However, our intonation was proven wrong by Rafal et al. (2017). They examined the effect of dropping the results of
    stragglers without using backup workers. Having fewer machines implies a smaller effective mini-batch size and thus
    greater gradient variance, which requires more iterations for convergence.
</p>  
  
<center><div class="img-iterations">
    <img width="500" img src="images/iterations.png" alt="iterations" />
    <p>FIGURE 5: NUMBER OF ITERATIONS TO CONVERGE, by 
       <a href="https://arxiv.org/abs/1604.00981">Rafal et al. (2017)</a>
    </p>
</div></center>

<p>
 As shown in figure 5, the iterations needed to converge increase when reducing the number of workers we aggregate the gradient from. Thus,
  our approach reduces the iteration time per batch but increases the number of epochs to conversion and, therefore, the total training time.
 </p> 
<h3 id="backup-workers">Backup Workers</h3>
<p>
  Rafal et al. (2017) approached the straggler drawback using technics from other distributed systems
  (<a href="https://www.ibm.com/topics/mapreduce#:~:text=MapReduce%20is%20a%20programming%20paradigm,tasks%20that%20Hadoop%20programs%20perform.">MapReduce</a>,
  <a href="https://www.microsoft.com/en-us/research/project/dryad/">Dryad</a>,
  <a href="https://hadoop.apache.org/">Hadoop</a>,
  and <a href="https://spark.apache.org/">Spark</a>).
  They choose to add b backup workers to the N workers. Once the root aggregator receives N inputs,
  it aggregates their gradient and updates the parameters. The slowest b workers’ gradients are dropped when they arrive.
  From figure 2, we can see that their experiment resulted in 80% of the 98th gradient arriving under 2s,
  whereas only 30% of the final gradient did. Furthermore, the time to collect the final few gradients grows exponentially,
  resulting in wasted idle resources and time expended to wait for the slowest gradients.
</p>
      
<p>
  As part of the research, they ran empirical comparisons of synchronous and 
  <a href="https://arxiv.org/abs/1609.08326">Asynchronous distributed stochastic gradient descent</a>  
  algorithms on the
  <a href="https://cloud.google.com/tpu/docs/inception-v3-advanced">Inception model</a>  
  (Szegedy et al., 2016) trained on 
  <a href="https://www.image-net.org/">ImageNet Challenge dataset</a>
  
  (Russakovsky et al., 2015).
</p>
      
 <center><div class="img-convergence">
    <img width="500" img src="images/convergence_plots.png" alt="convergence" />
    <p>FIGURE 6: CONVERGENCE OF SYNC-SGD AND ASYNC-SGD ON INCEPTION MODEL USING VARYING NUMBER OF MACHINES, by
       <a href="https://arxiv.org/abs/1604.00981">Rafal et al. (2017)</a>
    </p>
</div></center>
 
<p>
  From figure 6, we can see how using different numbers of workers and backup workers (N+b) has a clear effect on
    conversion time and rate. Figure 6 (b), shows that fanning out more workers doesn’t necessarily provide better
    results. When passing ~105 workers, the model’s precision decreases by ~0.3%. Moreover, In figure 6 (c) and 6 (d),
    the improvement slope decrease as the number of workers increases, which is reflected in the “elbow” shape.
</p>    

<h2 id="Conclusion">Conclusion</h2>
<p>
  As the size of datasets and models’ complexity increases, distributed training will become more and more common in
    the MLOps ecosystem. As we saw, a delay in one training cycle has a high impact on the entire training process.
    Thus, the need to perfect the training process and reduce the time per epoch is high. If you have more ideas,
    questions, or thoughts — I’d love to hear about them!
      </p>
      
      
      
      
      
    </main>
  </body>
</html>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
