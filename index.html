

<!DOCTYPE html>
<html lang="en-US">
  <head>
    <link rel="icon" href="images/jender_eq.png">

    <meta charset="UTF-8">
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Artificial intelligence and Morality
Winter semester 2022 – 2023
</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta name="author" content="Yehuda Baharav" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="" />
<meta property="og:description" content="" />
<!-- End Jekyll SEO tag -->

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <meta property='og:title' content=''/>
    <meta property='og:description' content=''/>
    
    <link rel="stylesheet" href="stylesheet.css">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->

<!-- end custom head snippets -->

  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">Breaking the Bias:</h1>
      <h1 class="project-name">Recognize and Avoid Gender Bias in Recruiting Ads</h1>

      <h2 class="curse-name">Artificial intelligence and Morality
      </h2>
      <h2 class="semester-name">Winter semester 2022 – 2023</h2>
      <h2 class="reichman-name">Reichman University</h2>
      <!-- <h2 class="project-tagline" color=white><a href="website_homepage.htm">Home</a> <a href="website_hills.htm">Hills Pupil Tailored Website</a></h2> -->
      <h2 class="project-tagline"><a href="https://www.linkedin.com/in/yehuda-baharav/"  style="color:#ffffff">Yehuda Baharav - Efi Arazi School of Computer Science</a>
      <h2 class="project-tagline"><a href="https://www.linkedin.com/in/tomer-bar-tal-516127241"  style="color:#ffffff">Tomer Bar-Tal - Radzyner Law School</a>
      <h2 class="project-tagline"><a style="color:#ffffff">Shahaf Sherman - Radzyner Law School</a>
      <h2 class="project-tagline"><a style="color:#ffffff">Nizar Peter Assaf - Radzyner Law School</a>     
    </header>

    <main id="content" class="main-content" role="main">

This paper was written by 
      <a href="https://www.linkedin.com/in/yehuda-baharav/">Yehuda Baharav</a>, <a href="https://www.linkedin.com/in/tomer-bar-tal-516127241">Tomer Bar-Tal</a>, Shahaf Sherman and Nizar Peter Assaf from Reichman University.

<h2 id="Introduction">Introduction</h2>

<p>
Gender bias in job ads is a prevalent issue that has been extensively studied, particularly with regard to
  its impact on women<sup><a href="#ref1">[1]</a></sup>
  . Research has shown that women are more likely to apply for job postings only when
  they match the requirements almost perfectly, while men are more willing to apply even if they only
  partially meet the qualifications<sup><a href="#ref2">[2]</a></sup>  
</p>
<p>
Bias in recruitment ads can take different forms, including the use of gender-specific terms like Policeman or Fireman, or the use of language that is more appealing to one gender over the other . There is also a more subtle type of bias in the wording or phrasing of ads, which may not be intentionally offensive but can still be alienating to one gender, typically women. Examples of such wording include the use of phrases like "must have" or "commitment". 
</p>
<p>
To address this problem, we will build a website that will assist recruiters in writing gender-neutral job ads. The goal of the website is to help recruiters recognize and eliminate any gender bias in their job ads, thus increasing diversity and inclusivity in the workplace. 
</p>

<h2 id="methodology">Methodology</h2>
<p>
  In the initial phase of our research, we opted to construct an NLP (Natural Language Processing) model aimed at identifying instances of gender bias in recruitment advertisements.
 </p>
<strong>Data:</strong>      
<p>We obtained the data from <a href="https://pandologic.com/">Pandologic</a>, which consists of 2 tables:</p>
 <p>     
<li>
<strong>Jobs table</strong>, containing the job title, field, and description.
Its size is about <strong>50K samples</strong>.
</li>
<li>
<strong>Job seekers table</strong> that includes their first and last names, email addresses, physical addresses, and other personal information. This table also indicates the jobs for which each job seeker applied.
Its size is about <strong>15M samples</strong>.
</li>
 </p>
  

<ol>
<center><div class="image001">
    <img width="500" img src="images/image001.png" />
    <p>FIGURE 1: Jobs table sample (fabricated IDs)      
    </p>
</div></center>
  </ol>
      <p></p>
      <p></p>
<ol>
<center><div class="image002">
    <img width="500" img src="images/image002.png" />
    <p>FIGURE 2: Job seekers table sample (fabricated IDs and shuffled columns)      
    </p>
</div></center>
  </ol>
<p>To ensure <strong>information security</strong> and protect <strong>user privacy</strong>, we have not included all columns in the presentation of data. Additionally, (just for the presentation here) the columns have been deliberately mixed up and a fictitious CompPositionID has been used.</p>

<strong>Date engineering:</strong>
      
<p>We used the <a href="https://pypi.org/project/gender-guesser/">gender_guesser</a> library to extract the gender of users from the list of unique first names in the data. Afterward, we joined this information back to the users table.
  For example:</p>
<ol>
<center><div class="image003">
    <img width="500" img src="images/image003.png" />
    <p>FIGURE 3: Example of gender_guesser library
    </p>
</div></center>
  </ol>
      
 <strong>Bias index:</strong>    
 <p>
   To assess gender bias in job applications, we calculated a gender ratio by comparing the number of women who applied for each job to the number of men who applied for that same job. We then computed this ratio for each job field, such as lawyer or doctor, in our dataset.
</p>
<p> 
  Next, we developed a "bias index" that measures the variance or distance between the job ratio for each specific job and the average ratio for that job field. This index provides information about whether there is gender bias in the applicant pool for a particular job, relative to the overall ratio for that job field. The index ranges from -1 to 1, with -1 indicating bias against women, 1 indicating bias against men, and 0 indicating no bias.
</p>
<p> 
 To increase confidence in our analysis, we excluded job fields with fewer than 10 unique ads and jobs with fewer than 5 applicants.  
</p>

 <strong>Example:</strong> 
<p>
  Upon examining the jobs with the highest gender bias according to the index, we discovered several positions that exhibited a significant disparity in applications between men and women. For example, the attached  job of a Medical Office Manager demonstrated a gender gap,  with 51 applicants, all of them are male, while in average in this job field, 87% of the applicants are females.
</p>
<hr>    
<pre>
<strong>
Full Time Dental Office Manager - $25 to $30/hr + 401k, PTO & BONUSES
Our dental practice located in the adorable South Park area of San Diego, CA is seeking an experienced Dental Office Manager to <span style="background-color: #FFFF00">fearlessly</span> lead our team!
Our practice is <span style="background-color: #FFFF00">extremely busy and we don't have time to train</span>, so DENTAL OFFICE MANAGEMENT EXPERIENCE IS REQUIRED to be considered for this role.
For your <span style="background-color: #FFFF00">hard work and dedication</span> we offer:Competitive and generous pay 401KPaid Time OffPaid Holidays and Paid Sick LeaveQuarterly BonusesOur hours are Monday to Thursday 9AM to 6PM.
To be considered for our Dental Office Manager role please:
<span style="background-color: #FFFF00">Have at least</span> 3 years of office management experienceHave experience with Treatment PlanningHave HMO and PPO dental insurance billing experienceBe reliable and friendly.
Bilingual English/Spanish is a huge plus!This position is open now!  Apply today!
Skills:General Practice
</strong>
</pre>     
<hr>    

<p>Although this <strong>result may be considered an outlier</strong>, it is worth noting that bias likely persists in similar jobs, even if not to this extreme degree. It's important to acknowledge that the observed gender imbalance may be influenced by chance, <strong>but the overall trend remains significant</strong>.</p>
 <p><u>An interesting fact to note is that upon visiting the company's website, it was observed that the word 'fearlessly' has been removed from the job description since its initial posting:</u></p>     

<p><strong>EDA (exploratory data analysis):</strong></p>
      
<ol>
<center><div class="image004">
    <img width="500" img src="images/image004.png" />
    <p>FIGURE 4: Bias index density histogram 
    </p>
</div></center>
  </ol>

<p>The graph clearly shows that the bias against women in distinct areas is nearly twice as much as that against men in the corresponding areas. To provide a better explanation, we will mirror the right side of the graph to the left side:</p>
<ol>
<center><div class="image005_v2">
    <img width="500" img src="images/image005_v2.png" />
    <p>FIGURE 5: Bias index density histogram - Women VS Men 
    </p>
</div></center>
  </ol>
      
      
      
      
<strong>Transfer learning - NLP model:</strong>
<p>
Initially, We experimented with traditional machine learning techniques such as boosting. However, we quickly realized that this approach was not suitable, and therefore, we will not delve into it further. Instead, we opted for using a pre-trained model
<a href="https://huggingface.co/distilbert-base-uncased">distilbert-base-uncased</a> by dropping the last layer, and training it on our data. If required, we would also train a few more layers backwards.
</p>
<p>
  The rationale behind using a pre-trained model is that in deep learning, the semantic meaning becomes more complex as we move deeper into the layers. Hence, the initial layers can be used for nearly any purpose in the field, such as text analysis in our case. The only aspect that needs to be learned about our target is limited to the last few layers.
</p>
<p>
We will explain this concept using image analysis as it is difficult to illustrate visually with text analysis. In the initial stage, the model identifies more general features, such as differences between pixels in the image. Similarly, in text analysis, this could include identifying the language of the text or analyzing linking words. As the analysis progresses, the model delves deeper into more meaningful features, such as identifying the presence of a person in a specific area of the picture. In text analysis, this could mean analyzing the use of high language in a particular sentence compared to the sentences before and after it.
</p>
<ol>
  <center><div class="img-Sync-SGD-diagram">
    <img width="500" img src="images/image006.png" alt="Source: Image Recognition with Deep Neural Networks and its Use Cases" />
    <p>Source: <a href="https://www.altexsoft.com/blog/image-recognition-neural-networks-use-cases/">Image Recognition with Deep Neural Networks and its Use Cases</a></p>
</div></center>
</ol>
 <strong>Tokenizer:</strong>
 <p>To incorporate the description into the model, we needed to tokenize the text to extract features. Thanks to Hugging Face's AutoTokenizer, this process was automated according to the selected model, saving us a considerable amount of time.
      </p>
  <p>Tokenization is a crucial step in NLP, as it involves breaking down raw text into individual tokens (such as words or subwords) that can be further processed. However, selecting the right tokenizer can be challenging since different models and tasks may require different approaches.
      </p>
<p>AutoTokenizer solves this problem by automatically selecting the appropriate tokenizer based on the model architecture and task. By using AutoTokenizer, we were able to streamline the selection and configuration of the tokenizer, ultimately saving us time and effort.
  </p>
      
  <strong>Target definition and metrics: </strong>
<p>We explored various approaches for training our data. These included:</p>
 <p>     
<li>
Conducting a <strong>regression analysis on all available data</strong> and testing for general error (MAE). We also examined error concentration in problem areas, such as the end of the left tail where there is a bias. The advantage of this method is that we have a large amount of data, and we don't need to establish a threshold for identifying bias. However, the disadvantage is that the data is unbalanced, which could negatively impact performance in areas with bias.
  <p>The objective loss of this method was RMSE.</p>
</li>
<li>
Limiting our <strong>regression analysis to the area where bias exists</strong>, and balancing it with data of the same size from areas without bias. This approach balances the data, but it comes at the cost of losing some information.
  <p>The objective loss of this method was RMSE.</p>
</li>
<li>
Using a <strong>classification</strong> approach to train the model <strong>on the area where there is bias</strong> and balancing it with data of the same size from areas without bias. This method balances the data and may result in easier model training, but it requires establishing a threshold for identifying bias, and we still lose some data.
<p>Since the data is balanced in this case, we used RMSE as objective loss but also examined the confusion matrix of this method.</p>  
</li>
 </p>
 <p>
In summary, we explored different ways to train our data, each with its own set of advantages and disadvantages.
      </p>  
<h2 id="results">Results</h2>
      
<p>
  The initial model showed promising results, achieving over 90% accuracy across all methods. However, during one of the last tests, the model was run with a random target under the same distribution parameters as the original target, and the accuracy remained unchanged. After verifying that there was no correlation between the random target and the original target, our confidence in the model was lost, and it will not be deployed in our website.
      </p>

<ol>
<center><div class="image007">
    <img width="500" img src="images/image007.png" />
    <p>FIGURE 7: Model Accuracy over epochs
    </p>
</div></center>
  </ol>
      
<p>
One possible explanation for the model's high accuracy on a random target is that there may be many similar, albeit not identical, texts in the data. The model learns these texts during training, allowing it to predict answers accurately during validation.
</p>
<p>
 A naive solution to this problem is to set a difference threshold, only allowing texts that differ beyond this threshold to be entered. In other words, duplicates will be downloaded even if they are not 100% identical. However, this solution has a time complexity of O(n^2), making it impractical for large datasets such as the one being used (n is ~50K). 
</p>
Although there may be smarter solutions, we do not have enough time to implement them. Therefore, we will use results from previous studies in the field (not necessarily related to machine learning) and recommendations from international organizations to create a list of words that may contain gender bias. This list will then be embedded into our website.      
<p>
The websites and studies that we will use include:  
</p>
<p><a href="https://eige.europa.eu/publications/gender-sensitive-communication/practical-tools/examples-common-gendered-nouns-alternatives">Gender- sensitive Communication - European Institute for Gender Equality</a></p>
<p><a href="https://www.researchgate.net/publication/50303045_Evidence_That_Gendered_Wording_in_Job_Advertisements_Exists_and_Sustains_Gender_Inequality">Evidence That Gendered Wording in Job Advertisements Exists and Sustains Gender Inequality</a></p>
 
<h2 id="legal-opinion">Legal Opinion</h2>
      
<p>
While promoting diversity and inclusivity in the workplace is an important goal, there are potential legal and constitutional issues that may arise from using a website to assist recruiters in writing gender-neutral job ads.
</p>
Prohibition of Discrimination in Products.(The Israeli law of Prohibition of Discrimination in Products, Services and Entry into Places of Entertainment and Public Places, 2000.) Section 3[a] which prohibits discrimination on the part of those who provide products, public services or operate public places in providing products, public services, entry to public places or providing services in public places, on the grounds of a customer's race, religion, nationality, land of origin, sex, sexual orientation, political views, personal status or parenthood.
<p>
<strong>Privacy</strong>- AI system is trained on a dataset that contains personal information such as resumes. the system is used to screen job candidates, it may collect, store, and share personal information in a way that raises concerns about data protection and security.
</p>
<strong>Accuracy</strong>-  AI system is trained on a biased dataset  may amplify existing biases in hiring and create unfairly rejected. This can lead to inaccurate decisions ,be harmful to individuals who may miss out on job opportunities and organizations, which may miss out on diverse talent. <strong>We need to make sure that our system not reduce the conversion rate</strong>. 
<p>
<strong>Bias</strong>- AI systems that are trained on a biased dataset may amplify existing biases, leading to unfair decisions that disproportionately favor or disadvantage certain groups of people. For example, an AI system trained on resumes mostly for men may be more likely to recommend men for job opportunities and by that leading to a lack of gender diversity in the workplace.
</p>
<p>
<strong>Autonomy</strong>- AI systems used in job advertisement may have a high degree of autonomy, making decisions or taking actions without human oversight, which can reformat ADs based on a certain criteria. This can be difficult for a human to understand or explain why certain text removed or added. Software is capable of doing whatever it wants.
</p>
<p>
<strong>Contracts obligation</strong>- clearly outline the ethical guidelines for the use of AI in recruitment and HR programs, and by regularly monitoring and evaluating the performance of the AI systems to ensure compliance with these guidelines. <strong>Additionally, the responsible of the program should have a process in place for addressing and resolving any ethical concerns that may arise</strong>. 
</p>
<p>
<strong>Transparency</strong>- job advertisement should be transparent so that the decision-making process can be audited, and bias can be detected and corrected. This can be done by providing decisions, or actions and making the training data and model available for inspection.
</p>
<p>
<strong>Constitutional issues</strong>- limiting the free speech rights of recruiters. Recruiters may argue that they have a the right to express their opinions and use language as they see fit in their job ads. While the law protect free speech, it is not an absolute right, and there may be limits on how far recruiters can go in expressing their opinions in job ads. This could become a legal issue if recruiters feel that their free speech rights are being infringed upon by the use of the website. 
</p>
<p>
Another potential issue is that the website may be seen as favoring one gender over another, which could raise constitutional issues related to equal protection. If the website is perceived as biased towards one gender, it could lead to claims that it is discriminatory.
</p>
<p>
To sum up, Building a website that helps recruiters write gender-neutral job ads could be an effective way to tackle gender bias in recruitment. By providing guidance and tools to recruiters, you can help ensure that job ads are inclusive and appeal to a diverse pool of candidates.  Some possible features of the website could include:  Gender-neutral language suggestions: The website could provide alternative wording for gender-specific terms or phrases that may be inadvertently biased. For example, instead of using "Policeman," the website could suggest using "Police Officer" instead. Tone analysis: The website could use natural language processing (NLP) algorithms to analyze the tone of job ads and flag any language that may be unintentionally off-putting to a particular gender. For example, the use of aggressive or assertive language may be more appealing to men, while women may respond better to more collaborative or empathetic language. Job ad templates: The website could provide templates for different job categories that have been pre-optimized for inclusivity and neutrality. This could help ensure that recruiters are starting with a strong foundation and minimize the risk of unintentional bias creeping in. Feedback and analysis: The website could provide feedback to recruiters on how their job ads perform in terms of diversity and inclusivity. For example, the website could analyze the language used in the job ad and provide a breakdown of how many words are associated with each gender. This could help recruiters identify areas for improvement and track progress over time. Overall, a website that helps recruiters write gender-neutral job ads could be a powerful tool for promoting diversity and inclusivity in the workplace.
</p>
  
<h2 id="conclusion">Conclusion</h2>
<p>
In conclusion, our study has shown that gender bias in job ads is still prevalent, and it can have a significant impact on the diversity and inclusivity of the workplace. However, we have also demonstrated that there are tools and methodologies available to address this issue. By using NLP models and bias indexes, we can identify instances of gender bias in job postings and help recruiters to write more gender-neutral ads.
</p>
 <p>
Our website is a step towards creating a more diverse and inclusive workplace, and we hope that our work will encourage more companies to take action against gender bias. It is crucial to keep striving towards equal opportunities and treatment for all genders in the workplace, and we believe that our study can contribute to this goal.
</p>
<p>
We believe that it is technically feasible to create the desired model using the data we have tested. However, extensive research would be necessary, which exceeds the scope of the current course. We will be enthusiastic about pursuing further research on this project, either as part of future degree courses or projects, or during our free time.
</p>

<h2 id="our-website">Our Website</h2>
  
<p><a href="https://yehudab.pythonanywhere.com/wordsmatter/"><big>WordsMatter</big></a></p>
<p>For example, you can paste there this sentence:</p>
<p>"We are looking for a salesman. He should have sales experience."</p>
      
<h2 id="references">References</h2>  
<hr>
<ul>
  <li id="ref1"><a href="https://ideas.wharton.upenn.edu/wp-content/uploads/2018/07/Gaucher-Friesen-Kay-2011.pdf">Evidence That Gendered Wording in Job Advertisements Exists and Sustains Gender Inequality</a></li>
  <li id="ref2"><a href="https://www.researchgate.net/publication/228628110_Perceiving_Glass_Ceilings_Meritocratic_Versus_Structural_Explanations_of_Gender_Inequality_Among_Women_in_Science_and_Technology">Perceiving Glass Ceilings? Meritocratic versus Structural Explanations of Gender Inequality among Women in Science and Technology</a></li>
</ul>  

     
    </main>
  </body>
</html>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
